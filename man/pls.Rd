% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pls.R
\name{pls}
\alias{pls}
\title{Partial Least Squares (PLS) Regression}
\arguments{
\item{X}{Numeric matrix of predictors, or name of such an assay from \code{data} .
\code{NA}s are allowed.}

\item{Y}{Numeric vector or matrix of responses (for multi-response models),
or name of such an \code{assay} or
\code{colData} from \code{data}. \code{NA}s are allowed.}

\item{ncomp}{The number of components to include in the model. Default to 2.}

\item{scale}{Boleean. If scale = TRUE, each block is standardized to zero
means and unit variances (default: TRUE)}

\item{mode}{Character string. What type of algorithm to use, (partially)
matching one of \code{"regression"}, \code{"canonical"}, \code{"invariant"}
or \code{"classic"}. See Details.}

\item{tol}{Convergence stopping value.}

\item{max.iter}{Integer, the maximum number of iterations.}

\item{near.zero.var}{Boolean, see the internal \code{\link{nearZeroVar}}
function (should be set to TRUE in particular for data with many zero
values). Setting this argument to FALSE (when appropriate) will speed up the
computations. Default value is FALSE}

\item{logratio}{One of ('none','CLR'). Default to 'none'}

\item{multilevel}{Design matrix for repeated measurement analysis, where
multlevel decomposition is required. For a one factor decomposition, the
repeated measures on each individual, i.e. the individuals ID is input as
the first column. For a 2 level factor decomposition then 2nd AND 3rd
columns indicate those factors. See examples in \code{?spls}).}

\item{all.outputs}{Boolean. Computation can be faster when some specific
(and non-essential) outputs are not calculated. Default = \code{TRUE}.}

\item{formula}{(\code{X} and \code{Y} must be \code{NULL})
formula of form \code{LHS~RHS} (names of objects without quotations) where
\code{LHS} and \code{RHS} (in effect \code{Y} and \code{X}, respectively) are
 numeric matrices . \code{LHS} and \code{RHS} can also be an assay names from
 \code{data}. \code{LHS} can also be a numeric \code{colData} name from \code{data}.
 see examples.}

\item{data}{A \code{MultiAssayExperiment} object.}
}
\value{
\code{pls} returns an object of class \code{"mixo_pls"}, a list that
contains the following components:

\item{X}{the centered and standardized original predictor matrix.}
\item{Y}{the centered and standardized original response vector or matrix.}
\item{ncomp}{the number of components included in the model.}
\item{mode}{the algorithm used to fit the model.} \item{variates}{list
containing the variates.} \item{loadings}{list containing the estimated
loadings for the \eqn{X} and \eqn{Y} variates.} \item{names}{list containing
the names to be used for individuals and variables.} \item{tol}{the
tolerance used in the iterative algorithm, used for subsequent S3 methods}
\item{iter}{Number of iterations of the algorthm for each component}
\item{max.iter}{the maximum number of iterations, used for subsequent S3
methods} \item{nzv}{list containing the zero- or near-zero predictors
information.} \item{scale}{whether scaling was applied per predictor.}
\item{logratio}{whether log ratio transformation for relative proportion
data was applied, and if so, which type of transformation.}
\item{explained_variance}{amount of variance explained per component (note
that contrary to PCA, this amount may not decrease as the aim of the method
is not to maximise the variance, but the covariance between data sets).}
\item{input.X}{numeric matrix of predictors in X that was input, before any
saling / logratio / multilevel transformation.} \item{mat.c}{matrix of
coefficients from the regression of X / residual matrices X on the
X-variates, to be used internally by \code{predict}.}
\item{defl.matrix}{residual matrices X for each dimension.}
}
\description{
Function to perform Partial Least Squares (PLS) regression.
}
\details{
\code{pls} function fit PLS models with \eqn{1, \ldots ,}\code{ncomp}
components. Multi-response models are fully supported. The \code{X} and
\code{Y} datasets can contain missing values.

The type of algorithm to use is specified with the \code{mode} argument.
Four PLS algorithms are available: PLS regression \code{("regression")}, PLS
canonical analysis \code{("canonical")}, redundancy analysis
\code{("invariant")} and the classical PLS algorithm \code{("classic")} (see
References). Different modes relate on how the Y matrix is deflated across
the iterations of the algorithms - i.e. the different components.

- Regression mode: the Y matrix is deflated with respect to the information
extracted/modelled from the local regression on X. Here the goal is to
predict Y from X (Y and X play an asymmetric role). Consequently the latent
variables computed to predict Y from X are different from those computed to
predict X from Y.

- Canonical mode: the Y matrix is deflated to the information
extracted/modelled from the local regression on Y. Here X and Y play a
symmetric role and the goal is similar to a Canonical Correlation type of
analysis.

- Invariant mode: the Y matrix is not deflated

- Classic mode: is similar to a regression mode. It gives identical results
for the variates and loadings associated to the X data set, but differences
for the loadings vectors associated to the Y data set (different
normalisations are used). Classic mode is the PLS2 model as defined by
Tenenhaus (1998), Chap 9.

Note that in all cases the results are the same on the first component as
deflation only starts after component 1.

The estimation of the missing values can be performed by the reconstitution
of the data matrix using the \code{nipals} function. Otherwise, missing
values are handled by casewise deletion in the \code{pls} function without
having to delete the rows with missing data.

logratio transform and multilevel analysis are performed sequentially as
internal pre-processing step, through \code{\link{logratio.transfo}} and
\code{\link{withinVariation}} respectively.
}
\examples{
#' \dontrun{
library(mixOmicsData)
data(linnerud)
## ---------------- with X and Y as matrices
X <- linnerud$exercise
Y <- linnerud$physiological
pls.res1 <- pls(X=X, Y=Y)
plotVar(pls.res1)

## ---------------- formula method for matrices
## 'formula' argument should be explicitly mentioned (formula = ...)
## for correct method dispatch
pls.res2 <- pls(formula = Y ~ X)
## exclude calls and see if all outputs  are identical
identical(pls.res1[-1], pls.res2[-1])
#> TRUE
## ---------------- MultiAssayExperiment and assay names as X and Y
## 'data' argument should be explicitly mentioned for correct method dispatch
data("linnerud.mae")
pls.res3 <- pls(X='exercise', Y='physiological', data = linnerud.mae, mode = "regression")
identical(pls.res1[-1], pls.res3[-1])
#> TRUE

## ---------------- MultiAssayExperiment and formula with assay names
pls.res4 <- pls(formula = physiological ~ exercise, data = linnerud.mae)
identical(pls.res1[-1], pls.res4[-1])
#> TRUE

## ---------------- data=MultiAssayExperiment; X=assay and Y=colData
data("liver.toxicity")
data("liver.toxicity.mae")
toxicity.pls1 <- pls(data = liver.toxicity.mae,  formula = Dose.Group~gene, ncomp = 3)
toxicity.pls2 <- pls(data = liver.toxicity.mae,  Y='Dose.Group', X='gene', ncomp = 3)
identical(toxicity.pls1[-1], toxicity.pls2[-1])
#> TRUE

#' }
}
\references{
Tenenhaus, M. (1998). \emph{La regression PLS: theorie et
pratique}. Paris: Editions Technic.

Wold H. (1966). Estimation of principal components and related models by
iterative least squares. In: Krishnaiah, P. R. (editors), \emph{Multivariate
Analysis}. Academic Press, N.Y., 391-420.

Abdi H (2010). Partial least squares regression and projection on latent
structure regression (PLS Regression). \emph{Wiley Interdisciplinary
Reviews: Computational Statistics}, 2(1), 97-106.
}
\seealso{
\code{\link{spls}}, \code{\link{summary}}, \code{\link{plotIndiv}},
\code{\link{plotVar}}, \code{\link{predict}}, \code{\link{perf}} and
http://www.mixOmics.org for more details.
}
\author{
Sébastien Déjean, Ignacio González, Kim-Anh Lê Cao, Al J Abadi.
}
\keyword{multivariate}
\keyword{regression}
